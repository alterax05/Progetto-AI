{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modello AI per MNIST digit recognition\n",
    "Per installare le dipendenze necessarie definite in requirements.txt\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "oppure\n",
    "```bash\n",
    "pip install torch torch torchvision torchaudio onnx\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install torch torch torchvision torchaudio onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importo le librerie necessarie\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch import cuda\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imposto il device su GPU se disponibile\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scarico il dataset MNIST\n",
    "dataset = datasets.MNIST('dataset', download=True, train=True, transform=ToTensor())\n",
    "\n",
    "#Creo il dataloader\n",
    "dataloader = DataLoader(dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## La rete neurale\n",
    "Questo è la CNN (Convolutional Neural Network) che useremo.\n",
    "È composta da layer che eseguiranno una convoluzione, la funzione di attivazione ReLU ed infine una funzione lineare che riduce l'output alle 10 cifre del dataset.\n",
    "I layer che eseguono la convoluzione prendono in input un tensore (la nostra immagine) e applicano i filtri con una \"finestra\" 3\\*3. Il valore dei filtri verra calcolato dalla rete neurale durante la fase di \"backward propagation\".\n",
    "Il motivo per cui l'ultimo layer ha '128*(28-(2*3))*(28-(2*3))' come input in quanto il tensore è stato reso bidimensionale, ma ad ogni convoluzione allìimmagine è stato rimosso un pixel da ogni lato.\n",
    "La funzione di attivazione compie un compito molto importante. Come in una rete neurale naturale i neuroni possono essere stimolati o no da uno stimolo esterno, così i neuroni digitali si \"stimolano\" in relazione alla funzione di attivazione. Inoltre prende parte nella fase di \"backward propagation\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    #Constuttore\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3), # 32 filters (la parte che verrà addestrata) , 3x3 kernel\n",
    "            nn.ReLU(), # Funzione di attivazione (rectified linear activation function)\n",
    "            nn.Conv2d(32, 64, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, 3),\n",
    "            nn.Flatten(), # Appiattisco l'immagine\n",
    "            nn.Linear(64*(28-(2*3))*(28-(2*3)), 10), # 128 neuroni, 10 classes\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creo il modello e lo sposto sulla GPU se disponibile\n",
    "model = Model().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function\n",
    "La funzione di perdita (loss function), in questo caso `nn.CrossEntropyLoss()`, è usata durante l'addestramento di una rete neurale per misurare quanto bene il modello sta facendo le sue previsioni.\n",
    "\n",
    "Quando viene chiamata `loss_fn(outputs, targets)`, calcola la log probabilità delle classi (i 10 numeri) previste (usando `LogSoftmax`) e poi calcola la `NLLLoss` (Negative Log Likehood Loss) tra le previsioni e i veri valori.\n",
    "\n",
    "Durante l'addestramento, l'obiettivo è minimizzare questa funzione di perdita. Questo significa che si vuole ridurre la differenza tra ciò che il modello prevede e i valori reali. Quando la funzione di perdita è minima, il modello ha la migliore performance possibile sui dati di addestramento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definisco la funzione di perdita, buona per classificazione multiclasse\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "optimizer = Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzione di valutare il modello con dati che non ha mai visto\n",
    "def evaluate(model):\n",
    "    dataset = datasets.MNIST('dataset', download=True, train=False, transform=ToTensor())\n",
    "    dataloader = DataLoader(dataset, batch_size=64)\n",
    "    correct = 0\n",
    "    total = len(dataset)\n",
    "    for data, target in dataloader:\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        pred = model(data)\n",
    "        correct += (pred.argmax(1) == target).type(torch.float).sum().item()\n",
    "    print(f'Accuracy: {(correct/total)*100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fuzione per esportare il modello in formato ONNX\n",
    "def toONNX(model, filename):\n",
    "    #                        (batch, channel, width and height)\n",
    "    dummy_input = torch.randn(1, 1, 28, 28).to(device)\n",
    "    torch.onnx.export(model, dummy_input, filename, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Trainig Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trainig loop\n",
    "if __name__ == '__main__':\n",
    "    for epoch in range(10): # 10 epochs\n",
    "        for batch_idx, (data, target) in enumerate(dataloader): \n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            # Forward\n",
    "            pred = model(data).to(device)\n",
    "            loss = loss_fn(pred, target)\n",
    "\n",
    "            # Backward\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            # Update\n",
    "            optimizer.step()\n",
    "\n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f'Epoch: {epoch}, Loss: {loss.item()}')\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    torch.save(model.state_dict(), 'model.pth')\n",
    "\n",
    "    print('Done training')\n",
    "\n",
    "    evaluate(model)\n",
    "\n",
    "    toONNX(model, 'model.onnx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
